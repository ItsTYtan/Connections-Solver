{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6a81d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Generated Output ===\n",
      "[ \"BENT\", \"FACULTY\", \"FLAIR\", \"GIFT\", \"PLAYWRIGHT\", \"SWORD\", \"WRAP\", \"WREATH\", \"DEAN\", \"GABLE\", \"GARLAND\", \"TEMPLE\", \"HAY\", \"JACKPOT\", \"ROAD\", \"ROOF\"]\n",
      "\n",
      "def find_famous_words(words):\n",
      "    # Initialize an empty list to store the words\n",
      "    famous_words = []\n",
      "    \n",
      "    # Iterate through each word in the given list\n",
      "    for word in words:\n",
      "        # Check if the word starts with a capital letter and is at least 5 letters long\n",
      "        if word.istitle() and len(word) >= 5:\n",
      "            # Append the word to the list of famous words\n",
      "            famous_words.append(word)\n",
      "            \n",
      "    return famous_words\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from peft import PeftModel, PeftConfig\n",
    "import torch\n",
    "\n",
    "# === Paths ===\n",
    "base_model_name_or_path = \"Qwen/Qwen2.5-1.5B-Instruct\"   # or local path\n",
    "adapter_path = \"models/ablation-1/checkpoint-2508\"              # your LoRA/PEFT adapter directory\n",
    "\n",
    "# === Load tokenizer ===\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name_or_path)\n",
    "\n",
    "# === Load base model ===\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name_or_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# === Load adapter ===\n",
    "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "model.eval()\n",
    "\n",
    "# === Text generation pipeline ===\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# === Run inference ===\n",
    "prompt = '[ \"BENT\", \"FACULTY\", \"FLAIR\", \"GIFT\", \"PLAYWRIGHT\", \"SWORD\", \"WRAP\", \"WREATH\", \"DEAN\", \"GABLE\", \"GARLAND\", \"TEMPLE\", \"HAY\", \"JACKPOT\", \"ROAD\", \"ROOF\"]'\n",
    "outputs = generator(prompt, max_new_tokens=100, do_sample=True, temperature=0.7)\n",
    "\n",
    "print(\"\\n=== Generated Output ===\")\n",
    "print(outputs[0][\"generated_text\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
